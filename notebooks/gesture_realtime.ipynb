{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bec943",
   "metadata": {},
   "source": [
    "# Real-time ASL gesture predictor (Webcam)\n",
    "\n",
    "This notebook loads the top models (MIL + 1-2 deeper models) and runs a simple real-time predictor using your webcam. It uses MediaPipe Hands to extract landmarks, maintains a sliding buffer of frames to form a window, and computes predictions from the deeper model and MIL model, then averages their probabilities for an ensemble prediction.\n",
    "\n",
    "Notes:\n",
    "- This is a demo notebook for quick local testing inside a Python environment with OpenCV and MediaPipe installed.\n",
    "- The MIL model requires a bag of windows; for low-latency demo the bag is constructed by repeating the most recent window to the required bag size (you can replace this with a sliding-bag buffer if you prefer).\n",
    "- Adjust `MODEL_PATH_*` variables below to pick the exact top models you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ff8fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model load failed for ../Models/gesture_wlasl_mil_finetuned_mil.keras -> The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().\n",
      "Model load status: deeper= True deeper2= True mil= False\n"
     ]
    }
   ],
   "source": [
    "# Imports and model loading\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import sys, os\n",
    "sys.path.insert(0, os.getcwd())\n",
    "# Choose top models (MIL + 1-2 deeper models). Update paths if needed.\n",
    "MIL_MODEL = '../Models/gesture_wlasl_mil_finetuned_mil.keras'\n",
    "DEEPER_MODEL = '../Models/gesture_wlasl_deeper_encinit_long.keras'\n",
    "# Optional second deeper model (ensemble of 3) - set to None to skip\n",
    "DEEPER_MODEL_2 = '../Models/gesture_wlasl_deeper_encinit_long.bilstm.keras'\n",
    "MIL_LABELS = '../Models/gesture_wlasl_mil_finetuned_mil_labels.json'\n",
    "DEEPER_LABELS = '../Models/gesture_wlasl_deeper_encinit_long_labels.json'\n",
    "DEEPER2_LABELS = '../Models/gesture_wlasl_deeper_encinit_long_labels.json'\n",
    "\n",
    "# load labels\n",
    "with open(MIL_LABELS) as f:\n",
    "    mil_labels = json.load(f)\n",
    "with open(DEEPER_LABELS) as f:\n",
    "    deeper_labels = json.load(f)\n",
    "# deeper2 uses same label file here; adjust if different\n",
    "with open(DEEPER2_LABELS) as f:\n",
    "    deeper2_labels = json.load(f)\n",
    "\n",
    "# helper to attempt loading full model, else return None (we'll try to load weights later)\n",
    "def load_model_try(path):\n",
    "    try:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    except Exception as e:\n",
    "        print('Full model load failed for', path, '->', e)\n",
    "        return None\n",
    "\n",
    "deeper = load_model_try(DEEPER_MODEL)\n",
    "deeper2 = None\n",
    "if DEEPER_MODEL_2:\n",
    "    deeper2 = load_model_try(DEEPER_MODEL_2)\n",
    "mil = load_model_try(MIL_MODEL)\n",
    "\n",
    "# If MIL failed to load as full model, attempt to load weights into a rebuilt MIL architecture at runtime when we know shapes. We'll handle that in setup below.\n",
    "print('Model load status: deeper=', bool(deeper), 'deeper2=', bool(deeper2), 'mil=', bool(mil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0114cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757805593.049939 7184508 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1757805593.064632 7186325 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1757805593.069567 7186334 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe helper and preprocessing utilities\n",
    "import mediapipe as mp\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_landmarks(img):\n",
    "    # img must be BGR as from cv2\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "    if not res.multi_hand_landmarks:\n",
    "        return None\n",
    "    lm = res.multi_hand_landmarks[0]\n",
    "    feats = []\n",
    "    for p in lm.landmark:\n",
    "        feats.extend([p.x, p.y, p.z])\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def make_window_from_buffer(buf, window_len, feat_dim):\n",
    "    # buf: deque of recent frame feature vectors (most recent at end)\n",
    "    arr = np.array(list(buf)[-window_len:], dtype=np.float32)\n",
    "    # if features dim mismatch, pad or trim\n",
    "    if arr.shape[1] < feat_dim:\n",
    "        pad = np.zeros((arr.shape[0], feat_dim - arr.shape[1]), dtype=np.float32)\n",
    "        arr = np.concatenate([arr, pad], axis=1)\n",
    "    elif arr.shape[1] > feat_dim:\n",
    "        arr = arr[:, :feat_dim]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding MIL architecture and loading weights...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCode\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_tf_mil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_encoder\n\u001b[1;32m     25\u001b[0m encoder \u001b[38;5;241m=\u001b[39m build_encoder((win, feat), embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     26\u001b[0m bag_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(bag_size, win, feat), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Code'"
     ]
    }
   ],
   "source": [
    "# Setup shapes & possibly rebuild MIL if needed\n",
    "# Determine window length and feature dim using deeper model if loaded, else try to infer from deeper model file name or default to 16x126\n",
    "default_window = 16\n",
    "default_feat = 126\n",
    "if deeper is not None:\n",
    "    try:\n",
    "        win = int(deeper.input_shape[1])\n",
    "        feat = int(deeper.input_shape[2])\n",
    "    except Exception:\n",
    "        win, feat = default_window, default_feat\n",
    "else:\n",
    "    win, feat = default_window, default_feat\n",
    "\n",
    "# bag size we'll use for MIL inference (must match MIL training bag size for best behavior)\n",
    "bag_size = 32\n",
    "\n",
    "# If MIL model failed to load as full model, attempt to reconstruct and load weights using known shapes\n",
    "if mil is None:\n",
    "    print('Rebuilding MIL architecture and loading weights...')\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "    from Code.train_tf_mil import build_encoder\n",
    "    encoder = build_encoder((win, feat), embed_dim=128)\n",
    "    bag_input = tf.keras.layers.Input(shape=(bag_size, win, feat), dtype=tf.float32)\n",
    "    td = tf.keras.layers.TimeDistributed(encoder)(bag_input)\n",
    "    att_dense = tf.keras.layers.Dense(1)(td)\n",
    "    att = tf.keras.layers.Softmax(axis=1)(att_dense)\n",
    "    pooled = tf.keras.layers.Lambda(lambda x: tf.matmul(x[0], x[1], transpose_a=True), name='pool')([td, att])\n",
    "    pooled = tf.keras.layers.Reshape((128,))(pooled)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(pooled)\n",
    "    out = tf.keras.layers.Dense(len(mil_labels), activation='softmax')(out)\n",
    "    mil = tf.keras.Model(bag_input, out)\n",
    "    # try to load weights file (either .keras or .weights.h5)\n",
    "    try:\n",
    "        mil.load_weights(MIL_MODEL)\n",
    "    except Exception:\n",
    "        if os.path.exists(MIL_MODEL + '.weights.h5'):\n",
    "            mil.load_weights(MIL_MODEL + '.weights.h5')\n",
    "print('Using window_len=', win, 'feat=', feat, 'bag_size=', bag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7293cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time loop: capture webcam, extract landmarks, buffer windows, run deeper+MIL ensemble and display overlay\n",
    "from time import time\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise SystemExit('Cannot open webcam')\n",
    "\n",
    "frame_buffer = deque(maxlen=win)  # store last `win` feature vectors\n",
    "fps_t0 = time()\n",
    "fps_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # small mirror for usability\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        feats = extract_landmarks(frame)\n",
    "        if feats is not None:\n",
    "            frame_buffer.append(feats)\n",
    "        # default text\n",
    "        label_text = 'No hand'\n",
    "        if len(frame_buffer) >= win:\n",
    "            window = make_window_from_buffer(frame_buffer, win, feat)\n",
    "            # deeper prediction: expect input shape (batch, win, feat)\n",
    "            deeper_probs = deeper.predict(window[np.newaxis,...], verbose=0)[0] if deeper is not None else np.zeros((len(mil_labels),))\n",
    "            # map deeper probs to mil label space\n",
    "            mil_from_deeper = np.zeros(len(mil_labels), dtype=float)\n",
    "            for name, di in deeper_labels.items():\n",
    "                di = int(di)\n",
    "                if name in mil_labels and di < len(deeper_probs):\n",
    "                    mil_from_deeper[int(mil_labels[name])] = deeper_probs[di]\n",
    "            # build a bag by repeating current window to bag_size for demo\n",
    "            bag = np.repeat(window[np.newaxis,...], bag_size, axis=0)\n",
    "            mil_input = np.expand_dims(bag, axis=0)  # shape (1, bag_size, win, feat)\n",
    "            mil_probs = mil.predict(mil_input, verbose=0)[0]\n",
    "            avg_prob = (mil_from_deeper + mil_probs) / 2.0\n",
    "            pred_idx = int(np.argmax(avg_prob))\n",
    "            # find label name\n",
    "            pred_name = None\n",
    "            for k,v in mil_labels.items():\n",
    "                if int(v) == pred_idx:\n",
    "                    pred_name = k\n",
    "                    break\n",
    "            label_text = pred_name if pred_name is not None else 'unknown'\n",
    "        # overlay text\n",
    "        cv2.putText(frame, f'Pred: {label_text}', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2)\n",
    "        # fps\n",
    "        fps_count += 1\n",
    "        if time() - fps_t0 >= 1.0:\n",
    "            fps = fps_count / (time() - fps_t0)\n",
    "            fps_t0 = time()\n",
    "            fps_count = 0\n",
    "        cv2.imshow('ASL Realtime', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL-Translator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
